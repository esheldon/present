\documentclass{beamer}

\usepackage{beamerthemesplit}
\usepackage{verbatim}
\usepackage[normalem]{ulem}

\usepackage{xcolor}

\usepackage{hyperref}

\definecolor{gold}{rgb}{1.,0.84,0.}
\definecolor{brightred}{rgb}{1.,0.4,0.4}
\definecolor{mygray}{RGB}{200,200,200}
\definecolor{lightsteelblue}{RGB}{176,196,222}
\definecolor{lightskyblue}{RGB}{135,206,250}
\definecolor{cadetblue}{RGB}{95,158,160}

%\usetheme{default}
%\usecolortheme{mule}
% \definecolor{mgreen}{RGB}{0,80,0}
\definecolor{mgreen}{RGB}{0,100,50}
\definecolor{mseagreen}{RGB}{46,139,87}
\setbeamercolor{structure}{fg=mgreen}

\usefonttheme{serif}

%\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\newcommand{\mcal}{\textsc{metacalibration}}
\newcommand{\Mcal}{\textsc{Metacalibration}}

\expandafter\def\expandafter\insertshorttitle\expandafter{%
      \insertshorttitle\hfill%
        \insertframenumber\,/\,\inserttotalframenumber}

% suppress navigation bar
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}{}


\title{Weak Lensing Algorithms and Lessons Learned from Precursor Experiments}
\author{Erin Sheldon}
\institute{Brookhaven National Laboratory}
\date{Match 19, 2020}
%


\begin{document}

%\setbeamertemplate{background canvas}[vertical shading][bottom=mgray,top=mblack]

\setbeamerfont*{itemize/enumerate body}{size=\Large}
\setbeamerfont*{itemize/enumerate subbody}{parent=itemize/enumerate body}
\setbeamerfont*{itemize/enumerate subsubbody}{parent=itemize/enumerate body}

\frame{\titlepage}

%
\frame
{

    % \frametitle{Introduction}

    {\huge The charge from the organizing committee: }
    \newline

    \begin{quote}
    ``...to speak on algorithms and pipelines you have developed or
    used in other surveys, lessons learnt, particularly any unexpected
    challenges you have encountered, and to highlight what you would do
    differently or additionally with LSST in order to maximize scientific
    return.''
    \end{quote}


}

\frame
{

    \frametitle{Outline}

    %\setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item The challenge
        \item Lessons from precursor surveys
        \item What we in DESC are doing for weak lensing shear

    \end{itemize}

}

\frame
{
    \frametitle{The Challenge}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item We have set the bar extremely high.  We want to make full use of the
            great data we will receive from the Rubin Observatory.

        \item For example, for the weak lensing probe we need to recover the
            signal with an accuracy of about 0.1\%.  At the time of writing,
            the most accurate measurements in the literature are at the 1-2\%
            level.

        \item Other probes also have very tight requirements.

        \item In order to meet these goals we must put forth a huge effort,
            embrace novel ideas and work with great discipline.

    \end{itemize}

}

\frame
{
    \frametitle{Precursor Survey Work}

    \begin{itemize}

        \item SDSS shear pipeline development and lensing analysis

        \item BOSS target selection framework

        \item DES shear pipeline development and lensing analysis

        \item What did we learn from all this, and how are we implementing
            that knowledge for LSST?

    \end{itemize}

}

\frame
{
    \frametitle{Lesson: Identify the Critical Problems that are Unsolved}

    \begin{itemize}

        \item There are unsolved problems which {\em must be solved} if we are to succeed.

        \item Dedicate as many resources as possible to solving these problems

            \begin{itemize}
                \item This will require some {\em convincing}
            \end{itemize}

        \item Spend as little time as possible on solved problems (more on this later)

    \end{itemize}

}

\frame
{
    \frametitle{Example from My Experience}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item DES was proposed in about 2003 with weak lensing (WL) as a primary
            probe.  We thought we would soon have a WL method that
            would meet our needs.

        \item Weak lensing measurement is still not a fully solved problem in
            2020.  Only recently have candidate algorithms been introduced that
            can work in principle.

        % \item There are hundreds of people in the project whose science depends
        %     directly on the weak lensing calibration.

        \item  Over the years there have been at most a few persons full time
            sustained effort (FTE) spent on this problem within DES, at a given
            time.  At times it may have been more like 1.0, shared between a few
            people, as most people have multiple duties.

            %At times this effort has been {\em less than one}.

    \end{itemize}

}
\frame
{
    \frametitle{Effort on Unsolved Problems in LSST}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item The situation is very similar in LSST right now, with about 1.3
            FTE on WL calibration.

        \item We have good candidates (metacalibration, BFD) but limited person power.

        \item If we fail we must fall back to {\em calibration from simulations}.  More
            on this later.
            %which as far as I know would be unprecedented for a major physics experiment.

    \end{itemize}

}



\frame
{
    \frametitle{Lesson: Use Existing Solutions if they are Good Enough}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item These solutions might not fit perfectly, we may have to bend
            them out of shape or adapt to their idiosyncrasies

        \item In most cases it will save so much time and frustration that it
            is worth it

        \item There is often more than one solution available, especially in
            cases where the software industry has already solved the problem.

        \item There are unsolved problems which {\em must be solved} if we are
            to succeed.  We must put our effort towards those problems.

    \end{itemize}

}

\frame
{
    \frametitle{Lesson: Aggressively test algorithms with validation simulations}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item Build up simulations in such a way that we can toggle all the
            relevant features, independently if possible

        \item Turn on features one at a time until we a) find a problem which we
            will try to fix or b) we reach full planned simulation complexity
            and the algorithm provides acceptable accuracy.

        \item Each test must be much more precise than the expected precision
            of the measurements we plan to make on real data.  For example, we
            want to be confident the bias is less than our requirements with at
            least 99.7\% confidence.  This is necessary but can be resource
            intensive.

    \end{itemize}

}

\frame
{
    \frametitle{Lesson: Use best software practices, e.g. Unit Tests}

    \begin{itemize}

        \item Validation tests are in addition to unit tests, which may also be
            partly simulation based

        \item Use extensive unit tests and continuous integration (e.g. circleci
            on github)

    \end{itemize}

}

\frame
{

    \frametitle{Lesson: Confirmation bias is even more relevant for validation
    simulations than for real data analysis }

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item The process is by definition iterative, we look for problems, fix
            them and rerun.  It's easy to subconsciously create a situation
            within the simulation or analysis that artificially produces
            accurate results.

        \item We tend to stop when we measure no bias and move on to the next
            test.

        \item Don't waste your time thinking about 1 sigma error bars:  always
            quote least 99.7\% confidence regions.  We will run thousands of
            tests and 2 sigma fluctuations will be seen regularly. I've
            seen results move my more than 3 sigma.

    \end{itemize}

}

\frame
{

    \frametitle{There May be Some Remaining Bias in the Validation Sims}

    \begin{itemize}

        \item There may be some remaining biases in our measurements which we
            either have good reason to think we can't fix, or we don't
            have time; think of a year 5 release deadline.

        \item For lensing we have no calibration sources, so we may need to use
            simulations to make the correction.

        \item How should we approach these corrections?

    \end{itemize}

}


\frame
{

    \frametitle{Lesson: Calibration simulations are harder than good data analysis}

    \setbeamerfont*{itemize/enumerate body}{size=\small}

    \begin{itemize}

        \item Often with data analysis, we don't need to understand everything that
            we measured, or how it is measured.

            \begin{itemize}

                \item We don't do absolute photometric calibration by
                    simulating the instrument.  We calibrate to reference
                    sources in a regime where the machine response is
                    approximately linear.

            \end{itemize}

        \item On the other hand, if we instead need to use a sim for estimating
            a large correction factor, we must know what we are doing. We need
            to understand what we put into the sim and it must match the real
            world very well.

        \item Therefore we should put as much effort as possible into developing
            algorithms that require small corrections.

            \begin{itemize}
    
                \item If the correction is 0.5\% we will be less sensitive to the
                    inevitable errors in the simulations than if the correction is 10\%.

            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{What are we doing in DESC for weak lensing shear?}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item LSST DM needs the community to deliver state of the art algorithms.
            Shear algorithms are still an area of research.

        \item Pipeline scientists (PS) Sheldon, Becker and Armstrong
            are working on this at about 1.3 FTE
            
        \item DESC PS are technically not funded to do research, only to
            implement existing algorithms.  So the research happens outside of
            DESC or ``in our spare time''.


    \end{itemize}

}

\frame
{

    \frametitle{What are we doing in DESC for weak lensing shear?}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item We in DESC plan to deliver at least two algorithms.

            \begin{itemize}
                \item Metacalibration (DESC PS Sheldon \& Becker)
                \item BFD (DESC PS Armstrong)
            \end{itemize}

        \item I'm not going to go into details of our algorithms because they
            are not of general interest.
            \begin{itemize}

                \item Both methods work well enough for isolated sources.
                    
                \item Metacalibration includes a detection phase to deal with
                    blending (Sheldon et al. 2019).
            \end{itemize}

        \item What are we doing to make these work for Rubin Obs. data?

    \end{itemize}

}

\frame
{

    \frametitle{Coadding}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item We coadd in small regions of sky
            \begin{itemize}
                \item For lensing we need a {\em continuous PSF}.
                \item We must only coadd images that do not have an edge in the region of sky.
                \item We waste less if we use smaller regions

                \item Too small is difficult for metacal, as we also need to
                    rerun detection on sheared versions of the images. About an
                    arcminute is what we currently use, but we have not
                    optimized this.

                \item Technically, we may be able to simply redefine the patch size and remove CCD images
                  with edges, this is TBD.

            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{PSF Coadding}

    \begin{itemize}
        \item We coadd the PSF exactly the same way we coadd the images

        \item This is different than the stack PSF coadding, more on that later.
    \end{itemize}

}

\frame
{

    \frametitle{Noise propagation}

    \begin{itemize}

        \item We propagate a noise image through all of the relevant processing
            stages.

        \begin{itemize}

            \item Coadding produces correlated noise.

            \item Interpolation of bad regions produces biases and correlated
                noise

            \item Modern algorithms such as metacalibration and BFD can
                propagate these effects using the noise image in order to
                produce unbiased results, all else being equal.

        \end{itemize}


    \end{itemize}

}

\frame
{

    \frametitle{Noise propagation}

    \begin{itemize}

        \item For BFD the noise field should include the noise from undetected
            sources.  However, the extra variance can be included separately by
            measuring the effect of undetected sources in the data (Eckert, Bernstein et
            al. 2020, in prep).

        \item For metacal the noise field should match the background noise (Sheldon \& Huff 2017).
            Undetected sources are part of the signal.
            

    \end{itemize}

}


\frame
{

    \frametitle{Detection}


    \begin{itemize}

        \item We detect on a combined multi-band straight coadd, nominally
            $r+i+z$.

        \item We process objects in multiple bands simultaneously 
            \begin{itemize}
                \item At this stage metacal and BFD are working separately
                \item I think the details are mostly not of general interest, but
                    I'm be happy to discuss on slack.
            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{How are we in DESC doing the work?}

    \begin{itemize}

        \item All code for working with the stack, metacal, and the sims is
            open source on github, heavily unit tested with continuous
            integration.

    \end{itemize}

}


\frame
{

    \frametitle{Using Existing Solutions}

    \begin{itemize}

                \item Use the stack for everything we can
                    \begin{itemize}
                        \item Take the calibrated exposures as input
                        \item Use the stack coadding code
                        \item Use the stack detection code
                        \item Use the stack WCS code
                    \end{itemize}

    \end{itemize}

}
\frame
{

    \frametitle{Feed back our solutions when we can't}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}


                \item We had to write code to include the sub-pixel shifts in
                    the PSF coaddition.  Important because the resampling
                    causes a small amount of blurring and we need this to be in
                    the PSF.

                \item Not currently possible to propagate noise images through the image
                  CR interpolation, for technical reasons, so we are doing our own noise
                  propagation and interpolation.  We will work with DM to make
                  this a stack feature.

                \item We will need to mask bright stars, we hope we can pull code
                    from HSC to do this, but we may need something custom.



    \end{itemize}

}



\frame
{

    \frametitle{Using externally developed codes}

    \begin{itemize}

        \item Using existing external codes for metacalibration \texttt{ngmix + galsim}.  (Development
            officially happens outside of DESC)

        \item BFD is under development but should be ready for testing soon.

    \end{itemize}

}

\frame
{

    \frametitle{Aggressive testing with validation simulations}

    \begin{itemize}

        \item We are developing image simulations in parallel with the analysis code.
        \item We can toggle each feature independently.

    \end{itemize}

}

\frame
{

    \frametitle{Simulation Features}


    \begin{itemize}

                \item field dithers, rotations
                \item TAN WCS with variations in pixel scale and wcs shear
                \item cosmic rays, bad columns
                \item realistic galaxy size and mag distributions and bulge+disk+AGN model
                  (WeakLensingDeblending)

                \item More morphological complexity will be added, but for
                    modern algorithms this is not critical for most testing.

    \end{itemize}
}

\frame
{

    \frametitle{Simulation Features cont..}


    \begin{itemize}

        \item realistic star mags and galactic density variations following DC2
            (thanks to J. Sanchez)

        \item Primitive Star saturation and fake bleed trails

        \item TODO realistic astrometric distortions, more realistic bright
            star effects (Jim Chiang helping to use DC2 examples)

    \end{itemize}
}


\frame
{


    \frametitle{Example Galaxies}

    \begin{center}
        \includegraphics[width=0.6\textwidth]{{example-sim-crop.png}}
    \end{center}

}

\frame
{


    \frametitle{Example Sim with Artifacts}

    \begin{center}
        \includegraphics[width=0.6\textwidth]{{artifacts.png}}
        \includegraphics[width=0.30\textwidth]{{example_starbleed.png}}
    \end{center}
    

}



\frame
{


    \frametitle{Current Results}

    % \setbeamertemplate{background canvas}[vertical shading][bottom=white,top=white]
	% \setbeamercolor{palette primary}{fg=black,bg=white}

    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}

            %{\color{black}
            \item Turn on features one by one
            \item Use wide confidence regions (99.7\%)
            \item Results for metacalibration shown at the right
            %    }

        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{{mvals-fig.png}}
            \end{center}
            
        \end{column}
    \end{columns}


}


\end{document}
