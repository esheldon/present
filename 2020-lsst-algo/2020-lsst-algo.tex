\documentclass{beamer}

\usepackage{beamerthemesplit}
\usepackage{verbatim}
\usepackage[normalem]{ulem}

\usepackage{xcolor}

\usepackage{hyperref}

\definecolor{gold}{rgb}{1.,0.84,0.}
\definecolor{brightred}{rgb}{1.,0.4,0.4}
\definecolor{mygray}{RGB}{200,200,200}
\definecolor{lightsteelblue}{RGB}{176,196,222}
\definecolor{lightskyblue}{RGB}{135,206,250}
\definecolor{cadetblue}{RGB}{95,158,160}

%\usetheme{default}
%\usecolortheme{mule}
% \definecolor{mgreen}{RGB}{0,80,0}
\definecolor{mgreen}{RGB}{0,100,50}
\definecolor{mseagreen}{RGB}{46,139,87}
\setbeamercolor{structure}{fg=mgreen}

\usefonttheme{serif}

%\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\newcommand{\mcal}{\textsc{metacalibration}}
\newcommand{\Mcal}{\textsc{Metacalibration}}

\expandafter\def\expandafter\insertshorttitle\expandafter{%
      \insertshorttitle\hfill%
        \insertframenumber\,/\,\inserttotalframenumber}

% suppress navigation bar
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}{}


\title{Weak Lensing Algorithms and Lessons Learned from Precursor Experiments}
\author{Erin Sheldon}
\institute{Brookhaven National Laboratory}
\date{Match 19, 2020}
%


\begin{document}

%\setbeamertemplate{background canvas}[vertical shading][bottom=mgray,top=mblack]

\setbeamerfont*{itemize/enumerate body}{size=\Large}
\setbeamerfont*{itemize/enumerate subbody}{parent=itemize/enumerate body}
\setbeamerfont*{itemize/enumerate subsubbody}{parent=itemize/enumerate body}

\frame{\titlepage}

%
\frame
{

    % \frametitle{Introduction}

    {\huge The charge from the organizing committee: }
    \newline

    \begin{quote}
    ``...to speak on algorithms and pipelines you have developed or
    used in other surveys, lessons learnt, particularly any unexpected
    challenges you have encountered, and to highlight what you would do
    differently or additionally with LSST in order to maximize scientific
    return.''
    \end{quote}


}

\frame
{

    \frametitle{Outline}

    %\setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item The challenge

        \item Lessons from precursor surveys

        \item What we in DESC are doing for weak lensing shear measurement.

    \end{itemize}

}

\frame
{
    \frametitle{The Challenge}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item We have set the bar extremely high.  We want to make full use of the
            rich data we will take at Rubin Observatory.

        \item For example, for the weak lensing probe we need to recover the
            signal with an accuracy of about 0.1\%.  At the time of writing,
            the most accurate measurements in the literature are at the 1-2\%
            level on real data.

        \item Other probes also have very tight requirements.

        \item In order to meet these goals we must put forth a huge effort,
            embrace novel ideas and work with great discipline.

    \end{itemize}

}

\frame
{
    \frametitle{Precursor Survey Work}

    \begin{itemize}

        \item SDSS shear pipeline development and lensing analysis

        \item BOSS target selection framework

        \item DES shear pipeline development and lensing analysis

        \item What did we learn from all this, and how are we applying this
            knowledge to our work for LSST?

    \end{itemize}

}

\frame
{
    \frametitle{Lesson: Identify the Critical Problems that are Unsolved}

    \begin{itemize}

        \item There are unsolved algorithmic problems which {\em must be
            solved} if we are to succeed.

        \item Dedicate as many resources as possible to solving these
            algorithmic problems

            \begin{itemize}
                \item This will require some {\em convincing}
            \end{itemize}

        \item Spend as little time as possible on solved problems (more on this later)

    \end{itemize}

}

\frame
{
    \frametitle{Example from My Experience}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item DES was proposed in about 2003 with weak lensing (WL) as a primary
            probe.  We thought we would soon have a WL method that
            would meet our needs.

        \item Weak lensing measurement is still not a fully solved problem in
            2020.  Only recently have candidate algorithms been introduced that
            can work in principle.

        % \item There are hundreds of people in the project whose science depends
        %     directly on the weak lensing calibration.

        \item  Over the years there have been at most a few persons full time
            sustained effort (FTE) spent on this problem within DES, at a given
            time.

        \item With hindsight we should have dedicated more effort to it.

            %At times this effort has been {\em less than one}.

    \end{itemize}

}
\frame
{
    \frametitle{Effort on Unsolved Problems in LSST}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item The situation is similar in LSST right now, with about 1.3
            FTE to make existing WL algorithms work for LSST (zero officially for
            research, PS are not funded to do research).

        \item We understand a lot more now.  We have good reason to {\em think}
            we have excellent candidates (BFD, \mcal\ etc) but....

        \item If we fail we must fall back to {\em calibration from simulations}.  More
            on this later.
            %which as far as I know would be unprecedented for a major physics experiment.

    \end{itemize}

}



\frame
{
    \frametitle{Lesson: Use Existing Solutions if they are Good Enough}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item These solutions might not fit perfectly, we may have to bend
            them out of shape or adapt to their idiosyncrasies

        \item In most cases it will save so much time that it is worth it

        \item There is often more than one solution available, especially in
            cases where the software industry has already solved the problem.

        \item There are unsolved problems which {\em must be solved} if we are
            to succeed.  We must put our effort towards those problems.

    \end{itemize}

}

\frame
{
    \frametitle{Lesson: Aggressively test algorithms with validation simulations}


    \begin{itemize}

        \item Build up simulations in such a way that we can toggle all the
            relevant features, independently if possible

        \item Turn on features one at a time until we a) find a problem which we
            will try to fix or b) we reach full planned simulation complexity
            and the algorithm provides acceptable accuracy.

        \item This requires dedication and effort that can equal or
            exceed the
            work on the algorithm itself.

    \end{itemize}

}

\frame
{
    \frametitle{Lesson: Aggressively test algorithms with validation simulations (cont.)}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}


        \item Each test must be much more precise than the expected precision
            of the measurements we plan to make on real data.  For example, we
            want to be confident the bias is less than our requirements with at
            least 99.7\% confidence.  This is necessary but can be resource
            intensive.

        \item Don't waste your time thinking about 1 sigma error bars:  always
            quote at least 99.7\% confidence regions.  We will run thousands of
            tests and 2 sigma fluctuations will be seen regularly. I've
            seen results move my more than 3 sigma.

            \begin{itemize}
                \item This applies to real data analysis too
            \end{itemize}

    \end{itemize}

}



\frame
{
    \frametitle{Lesson: Use best software practices, e.g. Unit Tests}

    \begin{itemize}

        \item Validation tests are in addition to unit tests, which may also be
            partly simulation based

        \item Use extensive unit tests and continuous integration (e.g. circleci
            on github)

    \end{itemize}

}

\frame
{

    \frametitle{Lesson: Confirmation bias is even more relevant for validation
    simulations than for real data analysis }

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item The process is by definition iterative, we look for problems, fix
            them and rerun.  It's easy to subconsciously create a situation
            within the simulation or analysis that artificially produces
            a zero bias result.

        \item We tend to stop when we measure no bias and move on to the next
            test.

        \item I'm not sure how to combat this except through diligence.
            \begin{itemize}

                \item We could try to have two completely independent simulation
                    packages, with one not influenced by the testing results.  But
                    this does not seem practical.

            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{There May be Some Remaining Bias in the Validation Sims}

    \begin{itemize}

        \item There may be some remaining biases in our measurements which we
            either have good reason to think we can't fix, or we don't
            have time; think of a year 1 cosmology deadline.

        \item For lensing we have no absolute calibration sources, so we may need to use
            simulations to make the correction.

        \item How should we approach these corrections?

    \end{itemize}

}


\frame
{

    \frametitle{Lesson: Calibration simulations are harder than good data analysis}

    \setbeamerfont*{itemize/enumerate body}{size=\small}

    \begin{itemize}

        \item Often with data analysis, we don't need to understand everything that
            we measured, or how it is measured.

            \begin{itemize}

                \item We don't do absolute photometric calibration by
                    simulating the instrument.  We calibrate to reference
                    sources.

            \end{itemize}

        \item If we instead need to use a sim for calibration, and the correction is large,
            we must {\em really} know what we are doing.

            \begin{itemize}
           
                \item We need to understand what we put into the sim and it
                    must match the real world very well.
            \end{itemize}

        \item Therefore we should put as much effort as possible into developing
            algorithms that require small corrections.

            \begin{itemize}
    
                \item If the correction is 0.5\% we will be less sensitive to the
                    inevitable errors in the simulations than if the correction is 10\%.

            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{What are we doing in DESC for weak lensing shear?}



    {\Large I'll first give the big picture of what we are doing and then give
    details and results toward the end }


}

\frame
{

    \frametitle{What are we doing in DESC for weak lensing shear?}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item LSST DM needs the community to deliver state of the art algorithms.
            Shear algorithms are still an area of research.

        \item Pipeline scientists (PS) Sheldon, Becker and Armstrong
            are working to implement existing algorithms at about 1.3 FTE
            
        \item (DESC PS are technically not funded to do research, only to
            implement existing algorithms.  So the research happens outside of
            DESC or ``in our spare time'').


    \end{itemize}

}




\frame
{

    \frametitle{What are we doing in DESC for weak lensing shear?}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item We in DESC plan to deliver at least two algorithms.

            \begin{itemize}
                \item BFD (DESC PS Armstrong)
                \item \Mcal\ (DESC PS Sheldon \& Becker)
            \end{itemize}

        \item I'm not going to go into details of our algorithms because they
            are not of general interest.
            \begin{itemize}

                \item Both methods work well enough for isolated sources.
                    
                \item \Mcal\ includes a detection phase to deal with
                    blending (Sheldon et al. 2019).
            \end{itemize}

        \item What are we doing to make these work for Rubin Obs. data?

    \end{itemize}

}

\frame
{

    \frametitle{Coadding}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}

        \item We coadd in small regions of sky
        \item For lensing we need a {\em continuous PSF}.
        \item We must only coadd images that do not have an edge in the region of sky.
        \item We waste less if we use smaller regions

            \begin{itemize}

                \item Technical points

                \item Too small is difficult for \mcal, as we also need to
                    rerun detection on sheared versions of the images. About an
                    arcminute is what we currently use, but we have not
                    optimized this.

                \item We may be able to simply redefine the patch size in the standard
                    DM code. This is TBD, currently we are defining the regions arbitrarily.

            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{PSF Coadding}

    \begin{itemize}

        \item We coadd the PSF exactly the same way we coadd the images, at
            least statistically.

        \item This is different than the stack PSF coadding, more on that later.

    \end{itemize}

}

\frame
{

    \frametitle{Noise propagation}

    \begin{itemize}

        \item We propagate a noise image through all of the relevant processing
            stages.

        \begin{itemize}

            \item Image resampling for coaddition produces correlated noise.

            \item Interpolation of bad regions produces biases and correlated
                noise

            \item Algorithms such as BFD and \mcal\ can
                propagate these effects using the noise image in order to
                produce unbiased results, all else being equal.

        \end{itemize}


    \end{itemize}

}

\frame
{

    \frametitle{Noise propagation}

    \begin{itemize}

        \item For BFD the noise field should include the noise from undetected
            sources.  However, the extra variance can be included separately by
            measuring the effect of undetected sources in the data (Eckert, Bernstein et
            al. 2020, in prep).

        \item For \mcal\ the noise field should match the background noise (Sheldon \& Huff 2017).
            Undetected sources are part of the signal.
            

    \end{itemize}

}


\frame
{

    \frametitle{Detection}


    \begin{itemize}

        \item We detect on a combined multi-band ``straight'' coadd, nominally
            $r+i+z$.

        \item We process objects in multiple bands simultaneously 

            \begin{itemize}

                \item Note after coadding BFD and \mcal\ work separately

            \end{itemize}

    \end{itemize}

}

\frame
{

    \frametitle{Details of how are we in DESC doing the work}

    \begin{itemize}

        \item Are we applying the lessons learned?

        \item How is the work going?

    \end{itemize}

}

\frame
{

    \frametitle{Details of how are we in DESC doing the work}

    \begin{itemize}

        \item All code for working with the stack, \mcal, and the sims is
            open source on github, heavily unit tested with continuous
            integration.

    \end{itemize}

}


\frame
{

    \frametitle{Using Existing Solutions}

    \begin{itemize}

                \item We use the stack for everything we can
                    \begin{itemize}

                        \item Take the calibrated exposures as input

                        \item Use the stack coadding code

                        \item Use the stack PSF

                        \item Use the stack detection code

                        \item Use the stack WCS code

                    \end{itemize}

    \end{itemize}

}
\frame
{

    \frametitle{Feed back our solutions when we can't}

    \setbeamerfont*{itemize/enumerate body}{size=\large}

    \begin{itemize}


                \item We had to write code to include the sub-pixel shifts in
                    the PSF coaddition.  Important because the resampling
                    causes a small amount of blurring and we need this to be
                    fully reflected in the PSF.

                \item Not currently possible to propagate noise images through the image
                  stack CR interpolation, for technical reasons, so we are doing our own noise
                  propagation and interpolation.  We will work with DM to make
                  this a stack feature.

                \item We will need to mask bright stars. It may be this can
                    happen down stream. We hope we can pull code from HSC to do
                    this, but we may need something custom.



    \end{itemize}

}



\frame
{

    \frametitle{Aggressive testing with validation simulations}

    \begin{itemize}

        \item We are developing image simulations in parallel with the analysis
            code (a good fraction of the 1.3 FTE goes towards the simulations)

        \item We can toggle each feature independently.

    \end{itemize}

}

\frame
{

    \frametitle{Current Simulation Features}


    \begin{itemize}

                \item field dithers, rotations

                \item TAN WCS with variations in pixel scale and wcs shear (need more realism)

                \item cosmic rays, bad columns

                \item Realistic galaxy size and mag distributions and bulge+disk+AGN model
                  (WeakLensingDeblending)

                  \begin{itemize}

                        \item More morphological complexity will be added, but
                            for modern algorithms this is not critical for most
                            testing.

                  \end{itemize}

    \end{itemize}
}

\frame
{

    \frametitle{Current Simulation Features cont..}


    \begin{itemize}

        \item Realistic multi-band star mags and galactic stellar density variations
            following DC2 (thanks to J. Sanchez)

        \item Primitive Star saturation and fake bleed trails

        \begin{itemize}
            \item TODO realistic astrometric distortions, more realistic bright
                star effects (Jim Chiang helping to use DC2 examples)
        \end{itemize}

    \end{itemize}
}


\frame
{


    \frametitle{Example Galaxies}

    Familiar to those who have used the WeakLensingDeblending package

    \begin{center}
        \includegraphics[width=0.6\textwidth]{{example-sim-crop.png}}
    \end{center}

}

\frame
{


    \frametitle{Example Sim with Artifacts}

    High stellar density field (80/sq arcmin)

    \begin{center}
        \includegraphics[width=0.6\textwidth]{{artifacts.png}}
        \includegraphics[width=0.30\textwidth]{{example_starbleed.png}}
    \end{center}
    

}



\frame
{


    \frametitle{Current Results}

    % \setbeamertemplate{background canvas}[vertical shading][bottom=white,top=white]
	% \setbeamercolor{palette primary}{fg=black,bg=white}

    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}

            %{\color{black}
            \item Turn on features one by one
            \item Use wide confidence regions (99.7\%)
            \item Results for \mcal\ shown at the right
            %    }

        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{{mvals-fig.png}}
            \end{center}
            
        \end{column}
    \end{columns}


}

\frame
{


    \frametitle{Example of something that needs exploration}

    % \setbeamertemplate{background canvas}[vertical shading][bottom=white,top=white]
	% \setbeamercolor{palette primary}{fg=black,bg=white}

    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}

            \item High stellar density is causing a bias

            \item Not clear yet what the cause is: for \mcal\ we would
                expect stars to produce a negative bias at much
                lower amplitude.

        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{{m-vs-density.png}}
            \end{center}
            
        \end{column}
    \end{columns}


}

\frame
{


    \frametitle{Summary}

    \begin{itemize} 

        \item It will be extremely challenging to utilize the full statistical power
            of the LSST data set.

        \item We are doing our best to apply the lessons learned from precursor surveys to
            LSST data processing.

        \item The weak lensing work is proceeding quickly and the results so far look
            very promising.

    \end{itemize} 

}



\end{document}
